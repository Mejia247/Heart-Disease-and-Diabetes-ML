# -*- coding: utf-8 -*-
"""hearts_Project_AI.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LqepPXlZTWWbdl0XAxKx_G7Rok6kWITx
"""

# Imports
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
from sklearn.model_selection import GridSearchCV
from sklearn.tree import DecisionTreeClassifier
from sklearn.tree import export_graphviz
import graphviz
from sklearn.metrics import accuracy_score
from sklearn.naive_bayes import GaussianNB
from sklearn.ensemble import RandomForestClassifier

# Cleaning Data

# Chaning so the model can train with binary lables. (Yes or No)

heart_df = pd.read_csv('heart_2020_cleaned.csv')
heart_df['Diabetic'] =  heart_df['Diabetic'].replace(['No, borderline diabetes'],'No') # Changes "No borderline diabetes" to just a No 
heart_df['Diabetic'] = heart_df['Diabetic'].replace(['Yes (during pregnancy)'],'Yes') # Changes the "yes (during pregancy)" to a yes
#heart_df['PhysicalHealth'].unique()

#heart_df.dtypes
heart_disease = heart_df['HeartDisease'] # This is the Label that says who has heart disease. We want to remove it from the dataset, so we don't include it into the training data.
heart_df.drop("HeartDisease",axis=1,inplace=True) # Drops the HeartDisease colomn 

heart_df

# This will turn the attubues into the Dummies
# Dummies make the lables into 1 or 0 (Computers can't humen text so it changes it to 1 and 0s.)
dumb = pd.get_dummies(heart_df) # creates the dummies
heart_df = dumb # This becomes the new dataframe with the dummies.

# Since floats can't be turned into dummies we will normilaze the data
x = heart_df[["BMI","PhysicalHealth","MentalHealth","SleepTime"]]
# Normilization
min_max = (x - x.min()) / (x.max()-x.min())
# Seting the normilzation into the dataframe
heart_df['BMI'] = min_max['BMI']
heart_df['PhysicalHealth'] = min_max['PhysicalHealth']
heart_df['MentalHealth'] = min_max['MentalHealth']
heart_df['SleepTime'] = min_max['SleepTime']


heart_df

# Fun part! This is were we will split the training data and testing Data
# This splits the data into testing and training sets, and it shuffles it too.
Xtrain, Xtest, ytrain, ytest = train_test_split(heart_df, heart_disease, test_size=0.2, random_state=13)


print('Xtrain.shape:', Xtrain.shape)
print('ytrain.shape:', ytrain.shape)
print('Xtest.shape:', Xtest.shape)
print('ytest.shape:', ytest.shape)

# This fits the model with the parameters.
model = KNeighborsClassifier(n_neighbors = 5, metric='euclidean',weights='uniform') # This the first attempt with random weight, metric and neighbors
model.fit(Xtrain,ytrain)

# y predictions 
ypred = model.predict(Xtest)
y_pred_training = model.predict(Xtrain)
ypred

ypred
y_pred_training

# seeing if ypred is equal to ytest
ypred == ytest

# This is the test/training accuracy BEFORE Grid SEARCH and the classifiction report
# *****************
print("Testing accuracy:",accuracy_score(ytest,ypred))
print("Training accuracy:",accuracy_score(ytrain,y_pred_training))
print(classification_report(ytest,ypred))

# This will use Grid seach to find the best paramters (Commeted out cause we do not want to run this again, since we got the parameters we needed. )

#param_grid = {'n_neighbors': np.arange(1, 51),
  #            'metric': ['euclidean','manhattan'],
 #             'weights': ['uniform','distance']}

#grid = GridSearchCV(model, param_grid, cv=5, verbose=1)
#grid.fit(Xtrain, ytrain)
#grid.best_params_

# we fit another model with the new parameters that we got from Gridsearch
model_2 = KNeighborsClassifier(n_neighbors = 31, metric='manhattan',weights='uniform') # This is the updated hyperperapaters after using GridSearch
model_2.fit(Xtrain,ytrain)

#parameters for 10,000 rows of data: {'metric': 'manhattan', 'n_neighbors': 31, 'weights': 'uniform'}

ypred_grid = model_2.predict(Xtest)
y_pred_training_grid = model_2.predict(Xtrain)
ypred

ypred_grid
y_pred_training_grid

ypred == ytest

# This is the test/training accuracy AFTER Grid SEARCH and classification report
#*****************
print("Testing accuracy:",accuracy_score(ytest,ypred_grid))
print("Training accuracy:",accuracy_score(ytrain,y_pred_training_grid))

"""Another Supervised Learning Technique. Decision Tree"""

# clf is using another classifcation called DecisionTree
clf = DecisionTreeClassifier(criterion='entropy',random_state=0)
clf = clf.fit(Xtrain,ytrain)
clf

# Visulization for the the Decision Tree
#*****************
vis_data = vis_data = export_graphviz(clf, out_file=None,
                feature_names=Xtrain.columns,
                class_names=['Heart_dieases_no', 'Heart_dieases_yes'],
                filled=True)
graph = graphviz.Source(vis_data)
graph

# ypred_2_testing and ypred_2_testing are new variables using the new Decision tree classification

ypred_2_test = clf.predict(Xtest)
ypred_2_training = clf.predict(Xtrain)

# Testing/training accuracy for Decision Tree Classification
print("Testing", accuracy_score(ytest,ypred_2_test))
print("Training", accuracy_score(ytrain,ypred_2_training))

# New Classifcation called Naive Baysian
clf_2 = GaussianNB()
clf_2.fit(Xtrain,ytrain)

ypred_3 = clf_2.predict(Xtest)
print(ypred_3)
print(ytest)

ypred_3_test = clf_2.predict(Xtest)
ypred_3_training = clf_2.predict(Xtrain)

# ********************
print("Testing", accuracy_score(ytest,ypred_3_test))
print("Training", accuracy_score(ytrain,ypred_3_training))